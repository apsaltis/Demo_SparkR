{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Initialize the Spark and SparkSQL contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(SparkR)\n",
    "sc <- sparkR.init(\"local[*]\")\n",
    "sqlCtx<- sparkRSQL.init(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Load the 3 datasets from Parquet files into SparkR as DataFrames\n",
    "- **txnsRaw**: Transaction data organized by line-item\n",
    "- **demo**: Demographic data, organized by customer ID\n",
    "- **sample**: A subset of customers who received a DM offer. This is our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txnsRaw<- loadDF(sqlCtx, paste(getwd(), \"/Customer_Transactions.parquet\", sep = \"\"), \"parquet\")\n",
    "demo <- withColumnRenamed(loadDF(sqlCtx, paste(getwd(), \"/Customer_Demographics.parquet\", sep = \"\"), \"parquet\"),\n",
    "                          \"cust_id\", \"ID\")\n",
    "sample <- loadDF(sqlCtx, paste(getwd(), \"/DM_Sample.parquet\", sep = \"\"), \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##We can view the schema of any DataFrame with `printSchema()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "printSchema(txnsRaw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Now we need to generate a few measures for customer behavior to use in our model.\n",
    "\n",
    "###Using our transactions data, we can create the follow variables:\n",
    "- **txns**: The number of transactions per customer\n",
    "- **spend**: Total expenditure per customer\n",
    "\n",
    "###To do this, we'll use the `groupBy` and `agg` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perCustomer <- agg(groupBy(txnsRaw,\"cust_id\"),\n",
    "                   txnsRaw$cust_id,\n",
    "                   txns = countDistinct(txnsRaw$day_num),\n",
    "                   spend = sum(txnsRaw$extended_price))\n",
    "\n",
    "head(perCustomer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##We also want to include demographic information about each customer in our model, so we'll need to join `perCustomer` to `demo` and select the relevant fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joinToDemo <- select(join(perCustomer, demo),\n",
    "                     demo$\"*\",\n",
    "                     perCustomer$txns, \n",
    "                     perCustomer$spend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Since we're using SparkSQL, our commands are being optimized by the Catalyst query optimizer. Let's take a look at Catalyst's plan for `joinToDemo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explain(joinToDemo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Wait, a Cartesian Product?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joinToDemo <- select(join(perCustomer, demo, perCustomer$cust_id == demo$ID),\n",
    "                     demo$\"*\",\n",
    "                     perCustomer$txns, \n",
    "                     perCustomer$spend)\n",
    "\n",
    "explain(joinToDemo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Now that we've got all our variables prepared, we need to create separate training and estimation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDF <- select(join(joinToDemo, sample, joinToDemo$ID == sample$cust_id),\n",
    "                joinToDemo$\"*\",\n",
    "                alias(cast(sample$respondYes, \"double\"), \"respondYes\"))\n",
    "\n",
    "estDF <- select(filter(join(joinToDemo, sample, joinToDemo$ID == sample$cust_id, \"left_outer\"),\n",
    "                      \"cust_id IS NULL\"),\n",
    "               joinToDemo$\"*\")\n",
    "\n",
    "printSchema(estDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Now that we've got our data prepped and pared down, we can turn each SparkSQL DataFrame into an R `data.frame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train <- collect(trainDF) ; train$ID <- NULL\n",
    "\n",
    "est <- collect(estDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Since we can seamlessly transition from Spark to R, we can make full use of SparkSQL's distributed query features while prepping our data and then switch to using R's native modeling features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theModel <- glm(respondYes ~ ., \"binomial\", train)\n",
    "\n",
    "summary(theModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Finally, let's create a custom scoring function that will use R's `predict` method and also output the scores by customer ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictWithID <- function(modObj, data, idField) {\n",
    "  scoringData <- data[, !names(data) %in% as.character(idField)]\n",
    "  scores <- predict(modObj, scoringData, type = \"response\", se.fit = TRUE)\n",
    "  idScores <- data.frame(ID = data[as.character(idField)], Score = scores$fit)\n",
    "  idScores[order( -idScores$Score), ]\n",
    "}\n",
    "\n",
    "testScores <- predictWithID(theModel, est, \"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(testScores, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
